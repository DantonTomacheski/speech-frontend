// src/components/RealTimeTranscription.tsx
import React, { useState, useRef, useEffect, useCallback } from 'react';
import { RecordingStatus, WebSocketMessage, TranscriptionData, ErrorData } from '../types'; // Importa tipos e enum
import MessageBox from './MessageBox';
import TranscriptionDisplay from './TranscriptionDisplay';
import Controls from './Controls';

// --- Constantes ---
// Certifique-se que a porta aqui corresponde ﾃ porta do backend (8081 no seu caso)
const WEBSOCKET_URL = import.meta.env.VITE_WEBSOCKET_URL || 'ws://localhost:8081';
const BUFFER_SIZE = 2048; // Tamanho do buffer de ﾃ｡udio

/**
 * @function RealTimeTranscription
 * @description Componente principal que gerencia a captura de ﾃ｡udio,
 * conexﾃ｣o WebSocket e estado da transcriﾃｧﾃ｣o em tempo real.
 * @returns {React.ReactElement} O elemento React.
 */
const RealTimeTranscription: React.FC = () => {
  // --- Estados ---
  const [status, setStatus] = useState<RecordingStatus>(RecordingStatus.Inactive);
  const [finalTranscription, setFinalTranscription] = useState<string>('');
  const [interimTranscription, setInterimTranscription] = useState<string>('');
  const [errorMessage, setErrorMessage] = useState<string | null>(null);

  // --- Refs ---
  const socketRef = useRef<WebSocket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const inputStreamRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const isRecordingRef = useRef<boolean>(false);

  // --- Funﾃｧﾃｵes de Callback ---

  const showMessage = useCallback((message: string) => {
    setErrorMessage(message);
    setTimeout(() => setErrorMessage(null), 5000);
  }, []);

  // Funﾃｧﾃ｣o de limpeza de ﾃ｡udio (mantida estﾃ｡vel com useCallback e sem dependﾃｪncias externas)
  const cleanupAudio = useCallback(() => {
    console.log("Limpando recursos de ﾃ｡udio...");
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current.onaudioprocess = null;
      processorRef.current = null;
    }
    if (inputStreamRef.current) {
      inputStreamRef.current.disconnect();
      inputStreamRef.current = null;
    }
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      // Verifica o estado antes de fechar para evitar warnings se jﾃ｡ estiver fechado
      audioContextRef.current.close().then(() => {
           console.log("AudioContext fechado.");
           audioContextRef.current = null;
      }).catch(console.error);
    } else {
         audioContextRef.current = null; // Garante que a ref seja limpa mesmo se jﾃ｡ estava fechado
    }
    isRecordingRef.current = false;
    console.log("Recursos de ﾃ｡udio limpos.");
  }, []); // Sem dependﾃｪncias externas, referﾃｪncia estﾃ｡vel

  // Funﾃｧﾃ｣o de fechamento do WebSocket (mantida estﾃ｡vel)
  const closeWebSocket = useCallback(() => {
    if (socketRef.current && socketRef.current.readyState === WebSocket.OPEN) {
      console.log("Enviando comando 'stopStreaming' e fechando WebSocket...");
      try {
        socketRef.current.send(JSON.stringify({ command: 'stopStreaming' }));
        socketRef.current.close(1000, "Client initiated stop");
      } catch (error) {
        console.error("Erro ao enviar comando/fechar WebSocket:", error);
      }
    }
    // Limpa a referﾃｪncia APﾃ鉄 tentar fechar, nﾃ｣o importa o estado
    socketRef.current = null;
  }, []); // Sem dependﾃｪncias externas, referﾃｪncia estﾃ｡vel

  // Funﾃｧﾃ｣o principal para parar a gravaﾃｧﾃ｣o
  // *** CORREﾃﾃグ: Removido 'status' das dependﾃｪncias ***
  const stopRecording = useCallback(() => {
    // Usa a ref para verificar o estado atual, mais confiﾃ｡vel que o estado React durante transiﾃｧﾃｵes rﾃ｡pidas
    if (!isRecordingRef.current) {
        console.log("Tentativa de parar gravaﾃｧﾃ｣o quando isRecordingRef.current ﾃｩ false.");
        return; // Evita chamadas mﾃｺltiplas ou em estados inadequados
    }
    console.log("Parando gravaﾃｧﾃ｣o...");
    setStatus(RecordingStatus.Stopping); // Atualiza o estado da UI para indicar parada
    isRecordingRef.current = false; // Atualiza a ref imediatamente

    cleanupAudio();
    closeWebSocket();

    // Atraso para garantir que a UI atualize antes de voltar para Inativo
    // Usamos um timeout para garantir que o estado 'Stopping' seja visﾃｭvel
    // e que as limpezas tenham chance de ocorrer antes de voltar para 'Inactive'.
    const timer = setTimeout(() => {
        // Verifica se ainda estamos parando e se o socket foi limpo
        // para evitar conflito se o usuﾃ｡rio clicar em iniciar novamente muito rﾃ｡pido
        if (!isRecordingRef.current && socketRef.current === null) {
             setStatus(RecordingStatus.Inactive);
             console.log("Gravaﾃｧﾃ｣o completamente parada. Estado: Inativo.");
        }
    }, 150);

    // Retorna uma funﾃｧﾃ｣o de limpeza para o caso de o componente ser desmontado
    // ou a funﾃｧﾃ｣o ser chamada novamente antes do timeout completar.
    return () => clearTimeout(timer);

  }, [cleanupAudio, closeWebSocket]); // Depende apenas das funﾃｧﾃｵes de limpeza estﾃ｡veis

  // --- Processamento de ﾃ「dio ---
  const handleAudioProcess = useCallback((event: AudioProcessingEvent) => {
    if (!isRecordingRef.current || !socketRef.current || socketRef.current.readyState !== WebSocket.OPEN) {
      return;
    }
    const audioData = event.inputBuffer.getChannelData(0);
    // Verifica se o socket ainda estﾃ｡ aberto antes de enviar (seguranﾃｧa extra)
    if (socketRef.current && socketRef.current.readyState === WebSocket.OPEN) {
       socketRef.current.send(audioData.buffer);
    }
  }, []); // Sem dependﾃｪncias externas, estﾃ｡vel

  // --- Inicializaﾃｧﾃ｣o do ﾃ「dio ---
  const initializeAudio = useCallback(async () => {
    // Adiciona verificaﾃｧﾃ｣o extra do estado do AudioContext
    if (isRecordingRef.current || (audioContextRef.current && audioContextRef.current.state !== 'closed')) {
        console.warn("Tentativa de inicializar ﾃ｡udio quando jﾃ｡ ativo ou contexto nﾃ｣o fechado.");
        // Se o contexto nﾃ｣o estiver fechado, tenta limpar antes de prosseguir
        if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
            cleanupAudio(); // Tenta limpar primeiro
            // Espera um pouco para a limpeza ocorrer antes de tentar novamente (opcional)
            await new Promise(resolve => setTimeout(resolve, 50));
        } else {
             return; // Evita inicializaﾃｧﾃ｣o mﾃｺltipla se isRecordingRef for true
        }
    }

    setStatus(RecordingStatus.Initializing);
    console.log("Inicializando ﾃ｡udio...");

    try {
      // Cria um NOVO AudioContext a cada inicializaﾃｧﾃ｣o para garantir um estado limpo
      const context = new AudioContext();
      audioContextRef.current = context; // Armazena a referﾃｪncia ao novo contexto

      // Verifica se o contexto precisa ser retomado (interaﾃｧﾃ｣o do usuﾃ｡rio pode ser necessﾃ｡ria)
      if (context.state === 'suspended') {
        console.log("AudioContext suspenso, tentando retomar...");
        await context.resume();
        console.log("AudioContext retomado.");
      }

      const stream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
      mediaStreamRef.current = stream;
      inputStreamRef.current = context.createMediaStreamSource(stream);

      // ATENﾃﾃグ: createScriptProcessor ﾃｩ obsoleto! Usar AudioWorklet em produﾃｧﾃ｣o.
      console.warn("[Deprecation] The ScriptProcessorNode is deprecated. Use AudioWorkletNode instead. (https://bit.ly/audio-worklet)");
      const processor = context.createScriptProcessor(BUFFER_SIZE, 1, 1);
      processor.onaudioprocess = handleAudioProcess;
      processorRef.current = processor;

      inputStreamRef.current.connect(processor);
      processor.connect(context.destination);

      isRecordingRef.current = true;
      setStatus(RecordingStatus.Recording);
      console.log("ﾃ「dio inicializado e gravaﾃｧﾃ｣o iniciada. Sample Rate:", context.sampleRate);
      // IMPORTANTE: Verifique se context.sampleRate corresponde ao `sampleRateHertz` no backend!

    } catch (err) {
      console.error('Erro ao inicializar ﾃ｡udio:', err);
      const error = err as Error;
      showMessage(`Erro ao acessar microfone: ${error.message}. Verifique as permissﾃｵes.`);
      setStatus(RecordingStatus.Error);
      cleanupAudio(); // Limpa recursos em caso de erro na inicializaﾃｧﾃ｣o
    }
  }, [showMessage, cleanupAudio, handleAudioProcess]); // Depende apenas de callbacks estﾃ｡veis

  // --- Conexﾃ｣o WebSocket ---
  // Deixamos 'status' aqui pois ﾃｩ lido no onclose.
  const connectWebSocket = useCallback(() => {
    if (socketRef.current) {
        console.warn("Tentativa de conectar WebSocket quando jﾃ｡ existe uma instﾃ｢ncia.");
        return;
    }
    setStatus(RecordingStatus.Connecting);
    console.log(`Conectando ao WebSocket em ${WEBSOCKET_URL}...`);

    setFinalTranscription('');
    setInterimTranscription('');

    try {
      const ws = new WebSocket(WEBSOCKET_URL);
      socketRef.current = ws;

      ws.onopen = () => {
        console.log('WebSocket conectado.');
        // Inicia ﾃ｡udio APﾃ鉄 conexﾃ｣o WS bem-sucedida
        initializeAudio();
      };

      ws.onmessage = (event) => {
        try {
          const data: WebSocketMessage = JSON.parse(event.data);
          if ('error' in data) {
            const errorData = data as ErrorData;
            console.error('Erro recebido do servidor:', errorData.error);
            showMessage(`Erro do servidor: ${errorData.error}`);
            stopRecording(); // Para tudo
          }
          else if ('transcript' in data) {
            const transcriptionData = data as TranscriptionData;
            if (transcriptionData.isFinal) {
              setFinalTranscription(prev => prev + transcriptionData.transcript + ' ');
              setInterimTranscription('');
            } else {
              setInterimTranscription(transcriptionData.transcript);
            }
          }
        } catch (error) {
          console.error('Erro ao processar mensagem do WebSocket:', error);
          showMessage('Erro ao processar resposta do servidor.');
        }
      };

      ws.onerror = (error) => {
        console.error('Erro no WebSocket:', error);
        showMessage('Erro na conexﾃ｣o WebSocket. Verifique se o servidor backend estﾃ｡ rodando.');
        setStatus(RecordingStatus.Error);
        // Nﾃ｣o chama stopRecording aqui diretamente, pois onclose serﾃ｡ chamado em seguida
      };

      ws.onclose = (event) => {
        console.log(`WebSocket desconectado: ${event.code} ${event.reason}`);
        // Limpa a referﾃｪncia do socket independentemente do motivo
        socketRef.current = null;
        // Verifica se a desconexﾃ｣o Nﾃグ foi iniciada pelo botﾃ｣o 'Parar' (cﾃｳdigo 1000)
        // e se o estado atual indica que deveria estar conectado/gravando.
        if (event.code !== 1000 && (status === RecordingStatus.Recording || status === RecordingStatus.Connecting || status === RecordingStatus.Initializing)) {
             setStatus(RecordingStatus.Disconnected);
             showMessage(`Conexﾃ｣o perdida: ${event.reason || 'Verifique o servidor'}`);
             // Chama cleanupAudio diretamente em vez de stopRecording para evitar loops
             cleanupAudio();
             setStatus(RecordingStatus.Inactive); // Volta para inativo apﾃｳs limpeza
        } else if (status !== RecordingStatus.Inactive && status !== RecordingStatus.Stopping) {
             // Se foi uma parada normal (cﾃｳdigo 1000) ou jﾃ｡ estava parando,
             // apenas garante que o estado final seja Inativo se ainda nﾃ｣o for.
             setStatus(RecordingStatus.Inactive);
        }
      };

    } catch (error) {
        console.error("Falha ao criar WebSocket:", error);
        showMessage("Nﾃ｣o foi possﾃｭvel conectar ao servidor WebSocket.");
        setStatus(RecordingStatus.Error);
        socketRef.current = null;
    }
  }, [initializeAudio, showMessage, stopRecording, status, cleanupAudio]); // Adicionado cleanupAudio

  // --- Efeito de Limpeza ---
  // *** CORREﾃﾃグ: Removido stopRecording das dependﾃｪncias ***
  // Este efeito agora sﾃｳ roda ao montar e desmontar o componente.
  useEffect(() => {
      return () => {
          console.log("Componente desmontando, limpando recursos...");
          // Chama as funﾃｧﾃｵes de limpeza diretamente ao desmontar
          cleanupAudio();
          closeWebSocket();
      };
  // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [cleanupAudio, closeWebSocket]); // Depende apenas das funﾃｧﾃｵes de limpeza estﾃ｡veis

  // --- Handlers de Botﾃ｣o ---
  const handleStart = () => {
    // Adiciona verificaﾃｧﾃ｣o de status para evitar cliques mﾃｺltiplos rﾃ｡pidos
    if (status !== RecordingStatus.Inactive && status !== RecordingStatus.Error && status !== RecordingStatus.Disconnected) {
        console.warn("Botﾃ｣o Iniciar clicado, mas o estado atual ﾃｩ:", status);
        return;
    }
    connectWebSocket(); // Inicia o processo
  };

  const handleStop = () => {
    // A verificaﾃｧﾃ｣o principal agora estﾃ｡ dentro de stopRecording usando isRecordingRef
    stopRecording(); // Chama a funﾃｧﾃ｣o de parada
  };

  // --- Renderizaﾃｧﾃ｣o ---
  return (
    <div className="bg-white p-8 rounded-lg shadow-lg w-full max-w-2xl mx-auto mt-10">
      <h1 className="text-2xl font-bold mb-6 text-center text-gray-800">
        痔 Transcriﾃｧﾃ｣o React em Tempo Real 痔
      </h1>

      <Controls
        onStart={handleStart}
        onStop={handleStop}
        // A lﾃｳgica de desabilitar botﾃｵes pode ser baseada apenas no status agora
        isRecording={status === RecordingStatus.Recording || status === RecordingStatus.Initializing}
        status={status}
      />

      <TranscriptionDisplay
        finalTranscription={finalTranscription}
        interimTranscription={interimTranscription}
      />

      <MessageBox message={errorMessage} onClose={() => setErrorMessage(null)} />
    </div>
  );
};

export default RealTimeTranscription;

